{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"name":"02-Optional_Scraping_Yelp.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"k7DLg7JStFPn"},"source":["# Scraping Yelp\n","\n","The aim of this exercise is to allow a user to make an automatic search on <a href=\"https://www.yelp.fr/\" target=\"_blank\">Yelp</a> and store the results in a `.json` file. You will be guided through the different steps: making a form request with search keywords, parsing the search results, crawling all the result pages and storing the results into a file.\n","\n","‚ö† **As scrapy is not made to launch several crawler processes in the same script, you will have to restart your notebook's kernel before completing each question!**"]},{"cell_type":"markdown","metadata":{"id":"O-tdRHOAtFPv"},"source":["1. Create a class `YelpSpider(scrapy.Spider)` with `start_urls = ['https://www.yelp.fr/']`. In this class, define a `parse(self, response)` method that automatically fills Yelp's homepage form with: \"restaurant japonais\" as search keywords and \"Paris\" as search location. Then, define another method `after_search(self, response)` that parses the first page of results, and yields the name and url of each search result. Finally, declare a `CrawlerProcess` that will store the results in a file named `\"restaurant_japonais-paris.json\"`."]},{"cell_type":"code","metadata":{"id":"OaXYt716tFPw"},"source":["# Import your libraries here\n","import os \n","import logging\n","import scrapy\n","from scrapy.crawler import CrawlerProcess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZU4yob5tFPw"},"source":["# Define your class YelpSpider(scrapy.Spider) with all methods needed\n","class YelpSpider(scrapy.Spider):\n","    name = \"yelp\"\n","    allowed_domains = ['www.yelp.fr']\n","    start_urls = ['https://www.yelp.fr/']\n","\n","    # Callback function that will be called when starting your spider\n","    # It will get text, author and tags of the first <div> with class=\"quote\"\n","    def parse(self, response):\n","        yield scrapy.FormRequest.from_response(response,\n","                                               formdata = {'find_desc':'restaurant japonais', 'find_loc':'Paris'},\n","                                               callback = self.after_search)\n","            \n","    def after_search(self, response):\n","        text_block = response.css('div.businessName__09f24__3Wql2.display--inline-block__09f24__FsgS4.border-color--default__09f24__R1nRO')\n","        for text in text_block:\n","            yield {\n","                'name': text.css('a.link__09f24__1kwXV.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::text').get(),\n","                'url': text.css('a.link__09f24__1kwXV.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::attr(href)').get()\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdYeWzpltFPx","outputId":"5b9f85bc-8ab5-48a6-e62b-5e6c7795a8cb"},"source":["# CrawlerProcess and settings go here\n","\n","filename = \"japanese-restaurant-paris.json\"\n","\n","# If file already exists, delete it before crawling (because Scrapy will \n","# concatenate the last and new results otherwise)\n","if not os.path.exists('./saving'):\n","    os.mkdir('./saving')\n","if filename in os.listdir('saving/'):\n","    os.remove('saving/' + filename)\n","\n","# Declare a new CrawlerProcess with some settings\n","## USER_AGENT => Simulates a browser on an OS\n","## LOG_LEVEL => Minimal Level of Log \n","## FEEDS => Where the file will be stored \n","## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n","process = CrawlerProcess(settings = {\n","    'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 Firefox/48.0',\n","    'LOG_LEVEL': logging.INFO,\n","    \"FEEDS\": {\n","        'saving/' + filename : {\"format\": \"json\"},\n","    },\n","    \"AUTOTHROTTLE_ENABLED\": True,\n","    \"COOKIE_ENABLE\": True\n","})\n","\n","# Start the crawling using the spider you defined above\n","process.crawl(YelpSpider)\n","process.start()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-12-13 15:20:58 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n","2020-12-13 15:20:58 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n","2020-12-13 15:20:58 [scrapy.crawler] INFO: Overridden settings:\n","{'AUTOTHROTTLE_ENABLED': True,\n"," 'LOG_LEVEL': 20,\n"," 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 '\n","               'Firefox/48.0'}\n","2020-12-13 15:20:58 [scrapy.extensions.telnet] INFO: Telnet Password: 4273b84827f6dcdc\n","2020-12-13 15:20:58 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats',\n"," 'scrapy.extensions.throttle.AutoThrottle']\n","2020-12-13 15:20:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2020-12-13 15:20:58 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2020-12-13 15:20:58 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2020-12-13 15:20:58 [scrapy.core.engine] INFO: Spider opened\n","2020-12-13 15:20:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2020-12-13 15:20:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2020-12-13 15:21:11 [scrapy.core.engine] INFO: Closing spider (finished)\n","2020-12-13 15:21:11 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: saving/japanese-restaurant-paris.json\n","2020-12-13 15:21:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 1983,\n"," 'downloader/request_count': 3,\n"," 'downloader/request_method_count/GET': 3,\n"," 'downloader/response_bytes': 131634,\n"," 'downloader/response_count': 3,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/302': 1,\n"," 'elapsed_time_seconds': 13.830109,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2020, 12, 13, 15, 21, 11, 988241),\n"," 'item_scraped_count': 10,\n"," 'log_count/INFO': 11,\n"," 'memusage/max': 76357632,\n"," 'memusage/startup': 76357632,\n"," 'request_depth_max': 1,\n"," 'response_received_count': 2,\n"," 'scheduler/dequeued': 3,\n"," 'scheduler/dequeued/memory': 3,\n"," 'scheduler/enqueued': 3,\n"," 'scheduler/enqueued/memory': 3,\n"," 'start_time': datetime.datetime(2020, 12, 13, 15, 20, 58, 158132)}\n","2020-12-13 15:21:11 [scrapy.core.engine] INFO: Spider closed (finished)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"PwXdaI0ytFPz"},"source":["2. Once you've managed to get the first page's results in `restaurant_japonais-paris.json`, complete the `after_search(self,response)` method to crawl the different result pages, such that all the search results will be stored in the file `\"restaurant_japonais-paris.json\"`. Restart your notebook's kernel, execute the new `CrawlerProcess` and check that all the search results (and not only the first page) are now stored in the file."]},{"cell_type":"code","metadata":{"id":"EINwBOEXtFPz"},"source":["# Import your libraries here\n","import os \n","import logging\n","import scrapy\n","from scrapy.crawler import CrawlerProcess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUkyu3CHtFP0"},"source":["# Define a new class YelpSpider based on the previous one \n","# but complete after_search method\n","class YelpSpider(scrapy.Spider):\n","    name = \"yelp\"\n","    allowed_domains = ['www.yelp.fr']\n","    start_urls = ['https://www.yelp.fr/']\n","\n","    # Callback function that will be called when starting your spider\n","    # It will get text, author and tags of the first <div> with class=\"quote\"\n","    def parse(self, response):\n","        yield scrapy.FormRequest.from_response(response,\n","                                               formdata = {'find_desc':'restaurant japonais', 'find_loc':'Paris'},\n","                                               callback = self.after_search)\n","            \n","    def after_search(self, response):\n","        text_block = response.css('div.businessName__09f24__3Wql2.display--inline-block__09f24__FsgS4.border-color--default__09f24__R1nRO')\n","        for text in text_block:\n","            yield {\n","                'name': text.css('a.link__09f24__1kwXV.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::text').get(),\n","                'url': text.css('a.link__09f24__1kwXV.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::attr(href)').get()\n","            }\n","        # Select the NEXT button and store it in next_page\n","        next_page = response.css('a.link__09f24__1kwXV.next-link.navigation-button__09f24__3F7Pt.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::attr(href)').get()\n","        # If a next page is found, execute the parse method once again\n","        try:\n","            yield response.follow(next_page, callback=self.after_search)\n","        except:\n","            logging.info('No next page. Terminating crawling process.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A73ZBXGOtFP0","outputId":"0bb85e85-7eb0-42f6-cf3b-058064ff04af"},"source":["# CrawlerProcess and settings go here\n","filename = \"japanese-restaurant-paris.json\"\n","\n","# If file already exists, delete it before crawling (because Scrapy will \n","# concatenate the last and new results otherwise)\n","if not os.path.exists('./saving'):\n","    os.mkdir('./saving')\n","if filename in os.listdir('saving/'):\n","    os.remove('saving/' + filename)\n","\n","# Declare a new CrawlerProcess with some settings\n","## USER_AGENT => Simulates a browser on an OS\n","## LOG_LEVEL => Minimal Level of Log \n","## FEEDS => Where the file will be stored \n","## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n","process = CrawlerProcess(settings = {\n","    'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 Firefox/48.0',\n","    'LOG_LEVEL': logging.INFO,\n","    \"FEEDS\": {\n","        'saving/' + filename : {\"format\": \"json\"},\n","    },\n","    \"AUTOTHROTTLE_ENABLED\": True,\n","    \"COOKIE_ENABLE\": True\n","})\n","\n","# Start the crawling using the spider you defined above\n","process.crawl(YelpSpider)\n","process.start()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-12-13 15:18:10 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n","2020-12-13 15:18:10 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n","2020-12-13 15:18:10 [scrapy.crawler] INFO: Overridden settings:\n","{'AUTOTHROTTLE_ENABLED': True,\n"," 'LOG_LEVEL': 20,\n"," 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 '\n","               'Firefox/48.0'}\n","2020-12-13 15:18:10 [scrapy.extensions.telnet] INFO: Telnet Password: c8fe3194fe365ec9\n","2020-12-13 15:18:10 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats',\n"," 'scrapy.extensions.throttle.AutoThrottle']\n","2020-12-13 15:18:10 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2020-12-13 15:18:10 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2020-12-13 15:18:10 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2020-12-13 15:18:10 [scrapy.core.engine] INFO: Spider opened\n","2020-12-13 15:18:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2020-12-13 15:18:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2020-12-13 15:19:10 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 22 pages/min), scraped 210 items (at 210 items/min)\n","2020-12-13 15:19:16 [root] INFO: No next page. Terminating crawling process.\n","2020-12-13 15:19:16 [scrapy.core.engine] INFO: Closing spider (finished)\n","2020-12-13 15:19:16 [scrapy.extensions.feedexport] INFO: Stored json feed (240 items) in: saving/japanese-restaurant-paris.json\n","2020-12-13 15:19:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 37608,\n"," 'downloader/request_count': 26,\n"," 'downloader/request_method_count/GET': 26,\n"," 'downloader/response_bytes': 1742133,\n"," 'downloader/response_count': 26,\n"," 'downloader/response_status_count/200': 25,\n"," 'downloader/response_status_count/302': 1,\n"," 'elapsed_time_seconds': 66.123385,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2020, 12, 13, 15, 19, 16, 860537),\n"," 'item_scraped_count': 240,\n"," 'log_count/INFO': 13,\n"," 'memusage/max': 117424128,\n"," 'memusage/startup': 76423168,\n"," 'request_depth_max': 24,\n"," 'response_received_count': 25,\n"," 'scheduler/dequeued': 26,\n"," 'scheduler/dequeued/memory': 26,\n"," 'scheduler/enqueued': 26,\n"," 'scheduler/enqueued/memory': 26,\n"," 'start_time': datetime.datetime(2020, 12, 13, 15, 18, 10, 737152)}\n","2020-12-13 15:19:16 [scrapy.core.engine] INFO: Spider closed (finished)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"33Un7D6htFP1"},"source":["Congrats, you've just made the proof of concept of making an automated search on Yelp with Scrapy! Now, let's improve the script such that it will allow the user to make any search at any location üòé\n","\n","3. Use python's `input()` function to ask the user which keywords and location he would like to use, and save them into two variables: `search_keywords` and `search_location`. Then, change the `parse(self, response)` method such that it fills Yelp's form with user-defined keywords and location. Finally, change the `CrawlerProcess` such that it stores the results in a file named with the following format : `search_keywords-location.json`. \n","\n","Try your search engine with different keywords and locations ‚úåÔ∏è"]},{"cell_type":"code","metadata":{"id":"7qpY7tEftFP2"},"source":["# Import your libraries here\n","import os \n","import logging\n","import scrapy\n","from scrapy.crawler import CrawlerProcess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCCbiP49tFP2","outputId":"ae398c1c-4104-4879-e492-6ab1be989a06"},"source":["# Use input() function here to define keywords\n","search_keywords = input(\"Enter keywords search: \")\n","search_location = input(\"Enter the location: \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Enter keywords search:  restaurants cubains\n","Enter the location:  Paris\n"],"name":"stdin"}]},{"cell_type":"code","metadata":{"id":"DT7zU3DFtFP3"},"source":["# Declare YelpSpider class here\n","class YelpSpider(scrapy.Spider):\n","    name = \"yelp\"\n","    allowed_domains = ['www.yelp.fr']\n","    start_urls = ['https://www.yelp.fr/']\n","\n","    # Callback function that will be called when starting your spider\n","    # It will get text, author and tags of the first <div> with class=\"quote\"\n","    def parse(self, response):\n","        yield scrapy.FormRequest.from_response(response,\n","                                formdata = {'find_desc':search_keywords, 'find_loc':search_location},\n","                                callback = self.after_search)\n","            \n","    def after_search(self, response):\n","        text_block = response.css('div.businessName__09f24__3Wql2.display--inline-block__09f24__FsgS4.border-color--default__09f24__R1nRO')\n","        for text in text_block:\n","            yield {\n","                'name': text.css('a.link__09f24__1kwXV.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::text').get(),\n","                'url': text.css('a.link__09f24__1kwXV.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::attr(href)').get()\n","            }\n","        # Select the NEXT button and store it in next_page\n","        next_page = response.css('a.link__09f24__1kwXV.next-link.navigation-button__09f24__3F7Pt.link-color--inherit__09f24__3PYlA.link-size--inherit__09f24__2Uj95::attr(href)').get()\n","        # If a next page is found, execute the parse method once again\n","        try:\n","            yield response.follow(next_page, callback=self.after_search)\n","        except:\n","            logging.info('No next page. Terminating crawling process.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzRNQwMYtFP3","outputId":"28d47223-f2a5-4d8b-a2d3-3bf338bf01b7"},"source":["# CrawlerProcess and settings go here\n","filename = (search_keywords + \"-\"+ search_location + \".json\").lower()\n","filename = filename.replace(\" \", \"-\")\n","\n","# If file already exists, delete it before crawling (because Scrapy will \n","# concatenate the last and new results otherwise)\n","if not os.path.exists('./saving'):\n","    os.mkdir('./saving')\n","if filename in os.listdir('saving/'):\n","    os.remove('saving/' + filename)\n","\n","# Declare a new CrawlerProcess with some settings\n","## USER_AGENT => Simulates a browser on an OS\n","## LOG_LEVEL => Minimal Level of Log \n","## FEEDS => Where the file will be stored \n","## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n","process = CrawlerProcess(settings = {\n","    'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 Firefox/48.0',\n","    'LOG_LEVEL': logging.INFO,\n","    \"FEEDS\": {\n","        'saving/' + filename : {\"format\": \"json\"},\n","    },\n","    \"AUTOTHROTTLE_ENABLED\": True,\n","    \"COOKIE_ENABLE\": True\n","})\n","\n","# Start the crawling using the spider you defined above\n","process.crawl(YelpSpider)\n","process.start()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-12-13 15:11:43 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n","2020-12-13 15:11:43 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n","2020-12-13 15:11:43 [scrapy.crawler] INFO: Overridden settings:\n","{'AUTOTHROTTLE_ENABLED': True,\n"," 'LOG_LEVEL': 20,\n"," 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 '\n","               'Firefox/48.0'}\n","2020-12-13 15:11:43 [scrapy.extensions.telnet] INFO: Telnet Password: 75f51826b1d82716\n","2020-12-13 15:11:43 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats',\n"," 'scrapy.extensions.throttle.AutoThrottle']\n","2020-12-13 15:11:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2020-12-13 15:11:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2020-12-13 15:11:43 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2020-12-13 15:11:43 [scrapy.core.engine] INFO: Spider opened\n","2020-12-13 15:11:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2020-12-13 15:11:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2020-12-13 15:12:05 [root] INFO: No next page. Terminating crawling process.\n","2020-12-13 15:12:05 [scrapy.core.engine] INFO: Closing spider (finished)\n","2020-12-13 15:12:05 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: saving/restaurants-cubains-paris.json\n","2020-12-13 15:12:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 5073,\n"," 'downloader/request_count': 5,\n"," 'downloader/request_method_count/GET': 5,\n"," 'downloader/response_bytes': 248646,\n"," 'downloader/response_count': 5,\n"," 'downloader/response_status_count/200': 4,\n"," 'downloader/response_status_count/302': 1,\n"," 'elapsed_time_seconds': 21.240298,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2020, 12, 13, 15, 12, 5, 97678),\n"," 'item_scraped_count': 25,\n"," 'log_count/INFO': 12,\n"," 'memusage/max': 76378112,\n"," 'memusage/startup': 76378112,\n"," 'request_depth_max': 3,\n"," 'response_received_count': 4,\n"," 'scheduler/dequeued': 5,\n"," 'scheduler/dequeued/memory': 5,\n"," 'scheduler/enqueued': 5,\n"," 'scheduler/enqueued/memory': 5,\n"," 'start_time': datetime.datetime(2020, 12, 13, 15, 11, 43, 857380)}\n","2020-12-13 15:12:05 [scrapy.core.engine] INFO: Spider closed (finished)\n"],"name":"stderr"}]}]}